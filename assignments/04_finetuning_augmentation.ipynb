{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Свёрточные сети для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "import torchmetrics\n",
    "from torch import Tensor, nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Callable\n",
    "from lightning.pytorch.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "from PIL.Image import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from typing import Any\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import torchmetrics.classification\n",
    "from typing import cast\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch.utilities.types import STEP_OUTPUT\n",
    "from torchmetrics.classification.confusion_matrix import ConfusionMatrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1. Skip-connections (2 балла)\n",
    "\n",
    "Постройте архитектуру свёрточной сети, аналогичную архитектуре в примере ниже, но добавьте в неё skip-connections, то есть дополнительные рёбра в вычислительном графе, позволяющие пропускать градиент в более ранние слои напрямую, минуя очередной блок Conv2D + BatchNorm + ReLU:\n",
    "\n",
    "```python\n",
    "def forward(self, x: Tensor) -> Tensor:\n",
    "    x = x + self.block1(x)\n",
    "    x = self.maxpool(x)\n",
    "    x = x + self.block2(x)\n",
    "    x = self.maxpool(x)\n",
    "    ...\n",
    "    x = x.adaptive_maxpool(x).flatten(1)\n",
    "    logits = self.fc(x)\n",
    "    return logits\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша верхнеуровневая архитектура будет выглядеть так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[nn.Module],\n",
    "        n_classes: int,\n",
    "        hidden_channels: list[int] = [32, 64],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        # входной слой, принимающий изображение с 3-мя каналами\n",
    "        self.in_conv = nn.Conv2d(3, hidden_channels[0], kernel_size=3, stride=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # собираем свёрточные блоки, каждый задаётся кол-вом входных и выходных каналов\n",
    "        blocks = []\n",
    "        for c_in, c_out in zip(hidden_channels[:-1], hidden_channels[1:]):\n",
    "            # добавляем очередной блок\n",
    "            blocks.append(block(c_in, c_out))\n",
    "            # добавляем Max pooling для уменьшения размерности\n",
    "            blocks.append(nn.MaxPool2d(2, 2))\n",
    "\n",
    "        # собираем блоки в единый Sequential модуль для удобства\n",
    "        self.features = nn.Sequential(*blocks)\n",
    "        self.maxpool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        # линейный слой для классификации\n",
    "        self.fc = nn.Linear(hidden_channels[-1], n_classes)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        h = self.features(self.relu(self.in_conv(x)))\n",
    "        logits = self.fc(self.maxpool(h).flatten(1))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовый блок, без residual connections, состоит из двух свёрток и нормализаций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, inplanes: int, planes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            inplanes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # first conv + bn + nonlinearity\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        # second conv + bn\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        # final nonlinearity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на результат его применения к тензору:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 32, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BasicBlock(4, 6).forward(torch.randn(3, 4, 32, 32)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нужно изменить этот блок, добавив в него skip-connection. Теперь в методе `forward` входной тензор `x` пойдёт по двум веткам:\n",
    "1. как в базовом блоке, через наши всёртки и нормализации, до последней нелинейности\n",
    "2. в обход свёрток и нормализаций\n",
    "\n",
    "В конце эти ветки нужно объединить через сумму. Тут есть проблема: в исходном тензоре `x` и обработанном нашим блоком `h(x)` отличается количество каналов (остальные размерности совпадают). То есть нам нужно сравнять количество каналов исходного тензора `inplanes` с количеством выходных каналов `outplanes`.\n",
    "\n",
    "Интуитивно, если рассматривать каждый пиксел входного тензора как вектор размера `inplanes`, в вектор размера `planes` его можно превратить домножением на матрицу размера `inplanes x planes`. Это можно сделать, создав свёрточный слой с размером кернела 1 - он и будет переводить наши пикселы в другую размерность.\n",
    "\n",
    "Не забудьте к сумме каналов применить нелинейность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inplanes: int, planes: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            inplanes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.trans = nn.Conv2d(inplanes, planes, kernel_size = 1, bias=False)\n",
    "        # добавьте свёртку 1x1 для изменения кол-ва каналов входного тензора\n",
    "        ...\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # сохраним входной тензор на будущее\n",
    "        identity = x\n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "\n",
    "        out += self.trans(identity)\n",
    "\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим размеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ResidualBlock(4, 6).forward(torch.randn(3, 4, 32, 32)).shape == torch.Size(\n",
    "    [3, 6, 32, 32]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что модель выдаёт тензор ожидаемого размера:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyResNet(ResidualBlock, 7, hidden_channels=[16, 32, 64, 128]).forward(\n",
    "    torch.randn(3, 3, 32, 32)\n",
    ").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем создавать модели разного размера, в том числе достаточно большие и глубокие, чтобы хорошо классифицировать изображения из датасета CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151047"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(\n",
    "    p.numel()\n",
    "    for p in MyResNet(ResidualBlock, 7, hidden_channels=[16, 32, 64, 64]).parameters()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2. Обучение `MyResNet` с использованием Lightning (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваша задача: добиться 80% точности на валидационной выборке с вашей реализацией `MyResNet`.\n",
    "\n",
    "После окончания обучения используйте метод `Trainer.validate` для вывода ваших метрик с удачного чекпоинта модели.\n",
    "\n",
    "NB: вызывайте `Trainer.validate` везде, где в задании требуется достичь какой-то точности\n",
    "\n",
    "\n",
    "Советы:\n",
    "- По умолчанию Lightning сохраняет только последний чекпоинт, так что вам может потребоваться `lightning.callbacks.ModelCheckpoint`, чтобы сохранять лучший чекпоинт в процессе обучения.\n",
    "\n",
    "- Используйте tensorboard, чтобы следить за динамикой обучения. Если заметите переобучение - подключайте регуляризацию. Большая модель с регуляризацией обычно лучше маленькой модели без неё.\n",
    "\n",
    "- Чтобы добиться нужной точности, ваша модель должна быть достаточно глубокой, ориентируйтесь на 4-5 блоков. Если необходимо, подключайте регуляризацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datamodule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        transform: Callable[[Image], Tensor] = transforms.ToTensor(),\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=self.transform,\n",
    "            )\n",
    "            self.val_dataset = datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=transforms.ToTensor(),\n",
    "            )\n",
    "        elif stage == \"validate\":\n",
    "            self.val_dataset = datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=transforms.ToTensor(),\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        # есть ещё стадии `test` и `predict`, но они нам не понадобятся\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/opt/homebrew/anaconda3/envs/dl-mcs/lib/python3.12/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model         | MyResNet         | 4.8 M  | train\n",
      "1 | train_metrics | MetricCollection | 0      | train\n",
      "2 | val_metrics   | MetricCollection | 0      | train\n",
      "-----------------------------------------------------------\n",
      "4.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.8 M     Total params\n",
      "19.310    Total estimated model params size (MB)\n",
      "36        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b85d6a54dce4cf3ad09cf1656520735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/dl-mcs/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/opt/homebrew/anaconda3/envs/dl-mcs/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841de1bb47cf415e8f456c364ef9a12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be3f75d2493488ca77142659d38384a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/dl-mcs/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "Epoch 0, global step 391: 'val_MulticlassAccuracy' reached 0.65750 (best 0.65750), saving model to './lightning_logs/version_89/checkpoints/best-checkpoint.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96605568766d472f8688b71a0493cbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 782: 'val_MulticlassAccuracy' reached 0.73090 (best 0.73090), saving model to './lightning_logs/version_89/checkpoints/best-checkpoint-v1.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7064d5dcc2a241bb90447ec983bbc586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 1173: 'val_MulticlassAccuracy' reached 0.81560 (best 0.81560), saving model to './lightning_logs/version_89/checkpoints/best-checkpoint.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e57902b41449a08bb721fd773a1690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 1564: 'val_MulticlassAccuracy' reached 0.83600 (best 0.83600), saving model to './lightning_logs/version_89/checkpoints/best-checkpoint-v1.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd7d521122a4328bb9a4bafcc0b40ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1955: 'val_MulticlassAccuracy' reached 0.84420 (best 0.84420), saving model to './lightning_logs/version_89/checkpoints/best-checkpoint.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716fbb88e14845fe992d098657825a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 2346: 'val_MulticlassAccuracy' reached 0.89140 (best 0.89140), saving model to './lightning_logs/version_89/checkpoints/best-checkpoint-v1.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf1f11280d544029d47af524e4d849b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 2737: 'val_MulticlassAccuracy' reached 0.89360 (best 0.89360), saving model to './lightning_logs/version_89/checkpoints/best-checkpoint.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a045c2c27146eaa1dbd258bf9f4ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 3128: 'val_MulticlassAccuracy' reached 0.89430 (best 0.89430), saving model to './lightning_logs/version_89/checkpoints/best-checkpoint-v1.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de292ad88cb5494ea24a7fa23df0c97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 3519: 'val_MulticlassAccuracy' reached 0.89540 (best 0.89540), saving model to './lightning_logs/version_89/checkpoints/best-checkpoint.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b4e7f6e3354a958739f029ed67d62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 3910: 'val_MulticlassAccuracy' reached 0.89730 (best 0.89730), saving model to './lightning_logs/version_89/checkpoints/best-checkpoint-v1.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec4a3c7807d42f8a8b2410c136e8816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 4301: 'val_MulticlassAccuracy' reached 0.89680 (best 0.89730), saving model to './lightning_logs/version_89/checkpoints/best-checkpoint.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f69c26f7f748bc9f04fc344d73cda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 4692: 'val_MulticlassAccuracy' was not in top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5343f3f3de4b6fb4b0c6a083442594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 5083: 'val_MulticlassAccuracy' reached 0.89740 (best 0.89740), saving model to './lightning_logs/version_89/checkpoints/best-checkpoint.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3fc7380b5b42daa196571ddb96ff26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 5474: 'val_MulticlassAccuracy' reached 0.89750 (best 0.89750), saving model to './lightning_logs/version_89/checkpoints/best-checkpoint-v1.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915dc39915084d988fde54bd99fcb13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 5865: 'val_MulticlassAccuracy' reached 0.89750 (best 0.89750), saving model to './lightning_logs/version_89/checkpoints/best-checkpoint.ckpt' as top 2\n",
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at ./lightning_logs/version_89/checkpoints/best-checkpoint-v1.ckpt\n",
      "Loaded model weights from the checkpoint at ./lightning_logs/version_89/checkpoints/best-checkpoint-v1.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49bd1b0281e34dbeaa6eb16c243e131c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "   val_MulticlassAUROC       0.993534505367279\n",
      " val_MulticlassAccuracy     0.8974999785423279\n",
      "        val_loss            0.33553123474121094\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.33553123474121094,\n",
       "  'val_MulticlassAccuracy': 0.8974999785423279,\n",
       "  'val_MulticlassAUROC': 0.993534505367279}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def create_classification_metrics(\n",
    "    num_classes: int, prefix: str\n",
    ") -> torchmetrics.MetricCollection:\n",
    "    return torchmetrics.MetricCollection(\n",
    "        [\n",
    "            torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes),\n",
    "            torchmetrics.classification.MulticlassAUROC(\n",
    "                num_classes=num_classes, average=\"macro\"\n",
    "            ),\n",
    "        ],\n",
    "        prefix=prefix,\n",
    "    )\n",
    "\n",
    "\n",
    "class Lit(L.LightningModule):\n",
    "    def __init__(self, model: nn.Module, learning_rate: float) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_metrics = create_classification_metrics(num_classes=10, prefix=\"train_\")\n",
    "        self.val_metrics = create_classification_metrics(num_classes=10, prefix=\"val_\")\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: tuple[Tensor, Tensor], batch_idx: int\n",
    "    ) -> STEP_OUTPUT:\n",
    "\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.train_metrics.update(y_hat, y)\n",
    "        self.log_dict(self.train_metrics, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: tuple[Tensor, Tensor], batch_idx: int\n",
    "    ) -> STEP_OUTPUT | None:\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        acc = self.val_metrics(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, on_step=False, prog_bar=True)\n",
    "        self.val_metrics.update(y_hat, y)\n",
    "        self.log_dict(self.val_metrics, on_step=False, on_epoch=True)\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"preds\": y_hat,\n",
    "        }\n",
    "\n",
    "    def configure_optimizers(self) -> dict[str, Any]:\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=1e-5)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": torch.optim.lr_scheduler.MultiStepLR(\n",
    "                optimizer, \n",
    "                milestones=[5, 10, 15]\n",
    "            ),\n",
    "        }\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "    monitor='val_MulticlassAccuracy', \n",
    "    mode='max',\n",
    "    save_top_k=2,\n",
    "    filename='best-checkpoint',\n",
    "    verbose=True\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_MulticlassAccuracy\",\n",
    "        mode=\"max\",\n",
    "        patience=5,\n",
    "    )\n",
    "]\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=15,\n",
    "    limit_train_batches=1000,\n",
    "    limit_val_batches=1000,\n",
    "    logger=TensorBoardLogger(save_dir=\".\"),\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "lit_module = Lit(\n",
    "    model=MyResNet(ResidualBlock, n_classes=10, hidden_channels=[64, 128, 256, 512]), learning_rate=0.001\n",
    ")\n",
    "datamodule = Datamodule(128)\n",
    "trainer.fit(model=lit_module, datamodule=datamodule,)\n",
    "trainer.validate(ckpt_path='best', model=lit_module, datamodule=datamodule,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3. Добавление аугментаций (1 балл + 2 балла за точность на валидации более 85%)\n",
    "\n",
    "Добавьте к обучающему датасету аугментации - случайные трансформации входных данных. Для этого можно использовать `torchvision.transforms` и `albumentations`.\n",
    "\n",
    "С `torchvision.transforms` совсем просто: вам нужно будет при создании `Datamodule` из практики по `lightning` указать вместо\n",
    "\n",
    "```python\n",
    "transform = transforms.ToTensor()\n",
    "```\n",
    "композицию трансформаций:\n",
    "\n",
    "```python\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # случайное зеркальное отражение\n",
    "    ...\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В пакете `albumentations` аугментаций значительно больше:\n",
    "\n",
    "![albumentations](https://albumentations.ai/assets/img/custom/top_image.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model         | MyResNet         | 1.2 M  | train\n",
      "1 | train_metrics | MetricCollection | 0      | train\n",
      "2 | val_metrics   | MetricCollection | 0      | train\n",
      "-----------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.838     Total estimated model params size (MB)\n",
      "36        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a95a849934d4c359e3afbcee58904a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c9b0907f52486dbc192302d6821e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e7ecdf606544d597dcd4bd5fe6e0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 196: 'val_MulticlassAccuracy' reached 0.67260 (best 0.67260), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b674b1704a4823a301cefee3f8dad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 392: 'val_MulticlassAccuracy' reached 0.57060 (best 0.67260), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint-v1.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddd19354c5e4d919ca64a81f6582293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 588: 'val_MulticlassAccuracy' reached 0.79230 (best 0.79230), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint-v1.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e196ac70b434855bfbfa4f9ef6b3e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 784: 'val_MulticlassAccuracy' reached 0.76390 (best 0.79230), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e758a2b53d454bc89267f9aed210c522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 980: 'val_MulticlassAccuracy' reached 0.82900 (best 0.82900), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d8039416874f75b22d331950b46717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1176: 'val_MulticlassAccuracy' reached 0.86390 (best 0.86390), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint-v1.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e69b1e3d48478e904bd94ef795f4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 1372: 'val_MulticlassAccuracy' reached 0.86820 (best 0.86820), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c71759e615405b8a62fdc6957292df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 1568: 'val_MulticlassAccuracy' reached 0.86960 (best 0.86960), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint-v1.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91b55423d604dbe95842f1b20eee633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 1764: 'val_MulticlassAccuracy' reached 0.87230 (best 0.87230), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6eea1507c2a4db78e15e784a64a837f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 1960: 'val_MulticlassAccuracy' reached 0.87550 (best 0.87550), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint-v1.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fdc9c722c74ae8acdd7a1f317a2cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 2156: 'val_MulticlassAccuracy' reached 0.87770 (best 0.87770), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0cdce5859045aaad00e2f796d09e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 2352: 'val_MulticlassAccuracy' reached 0.87810 (best 0.87810), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint-v1.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1926eb033fa54e9b809258ad119d49ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 2548: 'val_MulticlassAccuracy' reached 0.87930 (best 0.87930), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd323212210d40cda0ccd2074a27ff5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 2744: 'val_MulticlassAccuracy' reached 0.87920 (best 0.87930), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint-v1.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b69bb7ec0e40078e261a558a9dd124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 2940: 'val_MulticlassAccuracy' reached 0.87940 (best 0.87940), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint-v1.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3584f7d0ff044e49b7192865ce11ffe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 3136: 'val_MulticlassAccuracy' was not in top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f04981f7094e6a9e704ce3e0f44f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 3332: 'val_MulticlassAccuracy' was not in top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59cdd05f471f436194cc0291d72fb5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 3528: 'val_MulticlassAccuracy' reached 0.87970 (best 0.87970), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a50a9d61d424424a691cc5fdafe28b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 3724: 'val_MulticlassAccuracy' reached 0.88020 (best 0.88020), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint-v1.ckpt' as top 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d351b36a4f4bdd8bbe464a5d77fccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 3920: 'val_MulticlassAccuracy' reached 0.88040 (best 0.88040), saving model to './lightning_logs/version_90/checkpoints/best-checkpoint.ckpt' as top 2\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at ./lightning_logs/version_90/checkpoints/best-checkpoint.ckpt\n",
      "Loaded model weights from the checkpoint at ./lightning_logs/version_90/checkpoints/best-checkpoint.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0e571e68c445d1b543ab5bbbf5dab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "   val_MulticlassAUROC      0.9911805987358093\n",
      " val_MulticlassAccuracy     0.8804000020027161\n",
      "        val_loss            0.35570576786994934\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.35570576786994934,\n",
       "  'val_MulticlassAccuracy': 0.8804000020027161,\n",
       "  'val_MulticlassAUROC': 0.9911805987358093}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "    monitor='val_MulticlassAccuracy', \n",
    "    mode='max',\n",
    "    save_top_k=2,\n",
    "    filename='best-checkpoint',\n",
    "    verbose=True\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        patience=5,\n",
    "    )\n",
    "]\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=20,\n",
    "    limit_train_batches=1000,\n",
    "    limit_val_batches=1000,\n",
    "    logger=TensorBoardLogger(save_dir=\".\"),\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "lit_module = Lit(\n",
    "    model=MyResNet(ResidualBlock, n_classes=10, hidden_channels=[32, 64, 128, 256]), learning_rate=0.001\n",
    ")\n",
    "datamodule = Datamodule(256, transform=new_transform)\n",
    "trainer.fit(model=lit_module, datamodule=datamodule,)\n",
    "trainer.validate(ckpt_path='best', model=lit_module, datamodule=datamodule,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 4. Использование предобученной модели (4 балла)\n",
    "\n",
    "Теперь мы научимся использовать модели, обученные на других задачах\n",
    "\n",
    "Ваша задача: добиться 90% точности на тестовой выборке CIFAR-10. Постарайтесь уложиться модель с ~5 млн параметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В `torchvision.models` есть много реализованных архитектур, размером которых можно удобно управлять. Например, ниже можно создать крошечную версию модели `MobileNetV2`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46322"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import MobileNetV2\n",
    "\n",
    "mobilenet = MobileNetV2(\n",
    "    num_classes=10,\n",
    "    width_mult=0.4,\n",
    "    inverted_residual_setting=[\n",
    "        # t, c, n, s\n",
    "        [1, 16, 1, 1],\n",
    "        [3, 24, 2, 2],\n",
    "        [3, 32, 3, 2],\n",
    "    ],\n",
    "    dropout=0.2,\n",
    ")\n",
    "\n",
    "sum([param.numel() for param in mobilenet.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но кроме архитектуры модели, мы также можем скачать веса, полученные при обучении на каком-то датасете. Например, для нашей задачи можно использовать предобучение на самом известном датасете для классификации изображений - ImageNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4020358"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.efficientnet import EfficientNet_B0_Weights, efficientnet_b0\n",
    "\n",
    "# создаём EfficientNet с весами, полученными на ImageNet\n",
    "weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "efficientnet = efficientnet_b0(weights=weights)\n",
    "\n",
    "num_ftrs = efficientnet.classifier[1].in_features\n",
    "efficientnet.classifier[1] = nn.Linear(num_ftrs, 10)\n",
    "    \n",
    "\n",
    "first_param = 6\n",
    "for param in efficientnet.features[:first_param].parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "sum([param.numel() for param in efficientnet.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Указание 1.** С использованием модели в исходном виде есть проблема: в ImageNet 1000 классов, а у нас только 10. Поэтому в предобученной модели нужно будет полностью заменить последний линейный слой, который даёт распределение вероятностей классов. Это можно сделать уже в готовом объекте модели, переназначив атрибут.\n",
    "\n",
    "Подсказка: в `efficientnet_b0` линейный слой находится в атрибуте `classifier` \n",
    "\n",
    "\n",
    "**Указание 2.** Все слои, кроме нескольких последних (может быть, только последнего) мы можем заморозить, то есть сделать значения параметров в них неизменными. Это позволит и сохранить способность модели выделять полезные низкоуровневые признаки (она научилась этому на ImageNet), и существенно ускорить дообучение.\n",
    "\n",
    "\n",
    "Чтобы заморозить параметры, нужно всего лишь отключить для них расчёт градиентов. Вернитесь к первой практике, чтобы вспомнить, как это можно сделать. Нам подойдёт самый простой способ с `.requires_grad`.\n",
    "\n",
    "Подсказка: в `efficientnet_b0` свёрточные слои находятся в атрибуте `features` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Указание 3.** Предобученные модели на ImageNet ожидают специальным образом трансформированные изображения:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому эти трансформации нужно будет передать в датамодуль (как мы делали с аугментациями)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВАШ ХОД: Обучите модель и выведите результат метода validate на удачном чекпоинте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datamodule2(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        transform: Callable[[Image], Tensor] = weights.transforms(),\n",
    "        num_workers: int = 9,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        if stage == \"fit\":\n",
    "            self.train_dataset = datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=True,\n",
    "                download=True,\n",
    "                transform=self.transform,\n",
    "            )\n",
    "            self.val_dataset = datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=self.transform,\n",
    "            )\n",
    "        elif stage == \"validate\":\n",
    "            self.val_dataset = datasets.CIFAR10(\n",
    "                \"data\",\n",
    "                train=False,\n",
    "                download=True,\n",
    "                transform=self.transform,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type             | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model         | EfficientNet     | 4.0 M  | train\n",
      "1 | train_metrics | MetricCollection | 0      | train\n",
      "2 | val_metrics   | MetricCollection | 0      | train\n",
      "-----------------------------------------------------------\n",
      "3.2 M     Trainable params\n",
      "851 K     Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.081    Total estimated model params size (MB)\n",
      "343       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3f3811a114475fa2cd2e122969a40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/dl-mcs/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "/opt/homebrew/anaconda3/envs/dl-mcs/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57162064dc3e46e986e891337b0fba01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb795dc81a3d45bfb12d004ad9a94d32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 391: 'val_MulticlassAccuracy' reached 0.91790 (best 0.91790), saving model to './lightning_logs/version_91/checkpoints/best-checkpoint.ckpt' as top 2\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766cf3d81d024924ace572d5ad52a4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "   val_MulticlassAUROC      0.9959368109703064\n",
      " val_MulticlassAccuracy      0.917900025844574\n",
      "        val_loss            0.24704496562480927\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.24704496562480927,\n",
       "  'val_MulticlassAccuracy': 0.917900025844574,\n",
       "  'val_MulticlassAUROC': 0.9959368109703064}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "    monitor='val_MulticlassAccuracy', \n",
    "    mode='max',\n",
    "    save_top_k=2,\n",
    "    filename='best-checkpoint',\n",
    "    verbose=True\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        patience=3,\n",
    "    )\n",
    "]\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    max_epochs=1,\n",
    "    limit_train_batches=1000,\n",
    "    limit_val_batches=1000,\n",
    "    logger=TensorBoardLogger(save_dir=\".\"),\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "lit_module = Lit(\n",
    "    model=efficientnet, learning_rate=0.001\n",
    ")\n",
    "datamodule = Datamodule2(128)\n",
    "datamodule.setup(stage=\"fit\")\n",
    "\n",
    "trainer.fit(model=lit_module, datamodule=datamodule,)\n",
    "trainer.validate(model=lit_module, datamodule=datamodule,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-mcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
